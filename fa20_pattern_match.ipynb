{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DelEmptyString(strlist):\n",
    "    i = 0\n",
    "    while i < len(strlist):\n",
    "        if strlist[i] == None or len(strlist[i]) == 0:\n",
    "            del strlist[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return strlist\n",
    "\n",
    "def ProcessPattern(ptxt):\n",
    "    main_token = []\n",
    "    constraints = []\n",
    "    ### --- if ptxt == None or ptxt = \"\", return ---\n",
    "    if not ptxt or len(ptxt.strip()) == 0:\n",
    "        return main_token, constraints\n",
    "    ### --- split pattern_txt into words & delete the empty words ----\n",
    "    pwords = DelEmptyString(ptxt.split(' '))\n",
    "    ### --- divide the words into main_tokens and constraints ---\n",
    "    i = 0\n",
    "    while i < len(pwords):\n",
    "        temp_main = []\n",
    "        temp_constraints = []\n",
    "        ### ------- if current word is a constrain ----------\n",
    "        while i < len(pwords):\n",
    "            p = pwords[i]\n",
    "            ### if current word start with '&', append it into temp constraints;\n",
    "            if p[0] == '&':\n",
    "                temp_constraints.append(p)\n",
    "                i += 1\n",
    "            ### if current word start with '(', find the whole ignore pieces, append it into temp constraints\n",
    "            elif p[0] == '(':\n",
    "                if p[-1] == ')':\n",
    "                    temp_constraints.append(p)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    for j in range(i + 1, len(pwords)):\n",
    "                        p = p + ' ' + pwords[j]\n",
    "                        if pwords[j][-1] == ')':\n",
    "                            temp_constraints.append(p)\n",
    "                            i = j + 1\n",
    "                            break\n",
    "            else:\n",
    "                break\n",
    "        ### ------ if current word is a main_tokens --------\n",
    "        while i < len(pwords):\n",
    "            p = pwords[i]\n",
    "            if p[0] not in ['(', '&']:\n",
    "                ### ----- delete '/' ------\n",
    "                if '/' in p:\n",
    "                    temp_main.append(DelEmptyString(p.split('/')))\n",
    "                else:\n",
    "                    temp_main.append(p)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        constraints.append(temp_constraints)\n",
    "        if len(temp_main) > 0:\n",
    "            main_token.append(temp_main)\n",
    "\n",
    "    return main_token, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_token, constraints = ProcessPattern('&C (,/;/./--) consequently/test &THIS (,) &R')\n",
    "#re.compile(\"[.?!\\s]?([a-zA-z][a-zA-z\\s]+[.?!\\s]?) ([cC]onsequently), ([a-zA-z\\s]+[.?!])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['consequently', 'test']]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['&C', '(,/;/./--)'], ['&THIS', '(,)', '&R']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "### 'MainTokenRegExp' intend to: create regular expression list for all patterns\n",
    "### ---- in order to check whether the sentence contains a pattern's main_token ---\n",
    "### ---- notice that the capture group (.*) is in accordance with Patterns constraints ----\n",
    "def MainTokenRegExp(main_token, constraints):\n",
    "    ### current Regular Expression for current patterns\\\n",
    "    ### if current pattern only have one main_token, find the first appearance of this main_token ----\n",
    "    if len(main_token) == 1 and len(main_token[0]) == 1:\n",
    "        curRegExp = r'(.*?)'\n",
    "    ### else, try to match as much as the pattern can ---- \n",
    "    else:\n",
    "        curRegExp = r'(.*)'\n",
    "        ### patterns' main_token are broken into pieces, each piece must be matched successive.\n",
    "    for pi in range(len(main_token)):\n",
    "        ### ---- if current constraint pieces has a class constraint piece, add \" \" ----            \n",
    "        for cp in constraints[pi]: #cp is string\n",
    "            if cp[0] != '(': #TODO distinguish &C and &R\n",
    "                curRegExp = curRegExp + r'[a-zA-z][a-zA-z\\s]+'\n",
    "                #curRegExp = curRegExp + r' ' !TODO\n",
    "                break\n",
    "            else: #if constraint has '()'\n",
    "#                 curRegExp = curRegExp + cp\n",
    "                continue #TODO \n",
    "                #case 1 (,/./?) char\n",
    "                #case 2 (&AND) var\n",
    "                #case 3 \n",
    "        ### ---- add current main_token pieces into current Regular Expression -- curRegExp ----\n",
    "        for ti in range(len(main_token[pi])):\n",
    "            if ti > 0:\n",
    "                curRegExp += r' '\n",
    "            tokens = main_token[pi][ti]\n",
    "            ### if current token is a string, add it into curRegExp directly\n",
    "            if type(tokens) == str:\n",
    "                if tokens in ['.', '?', ':']:\n",
    "                    curRegExp += '\\\\'#.encode('utf-8') !! python3 return as bytes\n",
    "                curRegExp += tokens#.encode('utf-8')\n",
    "            ### if current token is a list, create a \"no capture group\" (?:token[0]|token[1]|...) for it\n",
    "            #eg reason/reasons\n",
    "            else:\n",
    "                curRegExp = curRegExp + r'(?:'\n",
    "                for tempt in range(len(tokens)):\n",
    "                    if tempt > 0:\n",
    "                        curRegExp = curRegExp + r'|'\n",
    "                    if tokens[tempt] in ['.', '?', ':']:\n",
    "                        curRegExp = curRegExp + '\\\\'#.encode('utf-8')\n",
    "                    curRegExp = curRegExp + tokens[tempt]#.encode('utf-8')\n",
    "                curRegExp = curRegExp + r')'\n",
    "        ### ---- if pattern is ended with a main_token, stop add \"(.*)\" ----\n",
    "        if len(main_token) == len(constraints) and pi == len(main_token) - 1:\n",
    "            break\n",
    "        ### ---- if next constraint pieces has a class constraint piece, add \" \" ----            \n",
    "        for cp in constraints[pi + 1]:\n",
    "            if cp[0] != '(':\n",
    "                #curRegExp = curRegExp + r' ' TODO \n",
    "                curRegExp = curRegExp + r'[a-zA-z][a-zA-z\\s]+'\n",
    "                break\n",
    "        ### ---- if current main_token pieces is not the last one, add (.*?), else add (.*) ----\n",
    "        if pi < len(main_token) - 1:\n",
    "            curRegExp = curRegExp + r'(.*?)'\n",
    "        else:\n",
    "            curRegExp = curRegExp + r'(.*)'\n",
    "\n",
    "    return re.compile(curRegExp, re.I)  ###re.I means ignore upper or lower cases\n",
    "\n",
    "#     Dumppickle(os.path.join(DICpkdir, 'mtRegExpList.pk'), mtRegExpList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'(.*?)[a-zA-z][a-zA-z\\s]+(?:consequently|test)[a-zA-z][a-zA-z\\s]+(.*)',\n",
       "re.IGNORECASE|re.UNICODE)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MainTokenRegExp(main_token, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manov script\n",
    "import argparse\n",
    "import re\n",
    "import csv\n",
    "#{&C (,/;/./--) consequently (,) &R}\n",
    "def parse(input_file, output_file):\n",
    "    txt = ''\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        txt = ''.join(f.readlines())\n",
    "    \n",
    "    ptrn = re.compile(\"[.?!\\s]?([a-zA-z][a-zA-z\\s]+[.?!\\s]?) ([cC]onsequently), ([a-zA-z\\s]+[.?!])\")\n",
    "    matches = ptrn.findall(txt)\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Cause\", \"Connector\", \"Effect\"])\n",
    "        writer.writerows([[matches[0], matches[1], matches[2]] for matches in matches])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Parsing arguments')\n",
    "    parser.add_argument('--input', type=str, help='The path to the input file \\\n",
    "        or directory')\n",
    "    parser.add_argument('--output', type=str, help='The path to the output file \\\n",
    "        or directory')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    parse(args.input, args.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qgnet)",
   "language": "python",
   "name": "qgnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
