{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !!!!!NEED TO run corenlp server and install package first, instructions are at\n",
    "# https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/\n",
    "\n",
    "# example for running server:\n",
    "# %%bash\n",
    "# cd /Users/emily/workspace/research/spring2020/stanford-corenlp-full-2018-10-05\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 30000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens\n",
    "\n",
    "\n",
    "sNLP = StanfordNLP()\n",
    "#     print(\"Annotate:\", sNLP.annotate(text))\n",
    "\n",
    "#     print(\"Parse:\", sNLP.parse(text))\n",
    "#     print(\"Dep Parse:\", sNLP.dependency_parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('wind', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('caused', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('storms', 'NNS')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sNLP.pos('the wind is caused by the storms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to textbook_context.txt for tony manov model\n",
    "# import json\n",
    "# import random\n",
    "\n",
    "# texts = []\n",
    "# f = open(\"textbook_context.txt\",\"w\") \n",
    "# with open('tqa_train_val_test/train/tqa_v1_train.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "#     for lesson in data:\n",
    "#         #one lesson\n",
    "        \n",
    "#         if len(texts)>=50:\n",
    "#             break\n",
    "#         #vocab\n",
    "#         vocabs = list(lesson[\"adjunctTopics\"][\"Vocabulary\"].keys())\n",
    "        \n",
    "#         if len(vocabs)!= 0 and random.uniform(0, 1)>0.5:\n",
    "#             #topics\n",
    "#             for tid in lesson[\"topics\"]:\n",
    "#                 if random.uniform(0, 1)>0.5:\n",
    "#                     text = lesson[\"topics\"][tid]['content']['text']\n",
    "#                     texts.append(text)\n",
    "#                     f.write(text)\n",
    "#                     f.write('\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tony and manov with vocab\n",
    "# import json\n",
    "# import random\n",
    "# import csv\n",
    "\n",
    "\n",
    "# text_ans = []\n",
    "# f = open(\"textbook_context_vocab.txt\",\"w\") \n",
    "# with open('tqa_train_val_test/train/tqa_v1_train.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "#     for lesson in data:\n",
    "#         #one lesson\n",
    "        \n",
    "#         if len(text_ans)>=50:\n",
    "#             break\n",
    "#         #vocab\n",
    "#         vocabs = list(lesson[\"adjunctTopics\"][\"Vocabulary\"].keys())\n",
    "        \n",
    "#         if len(vocabs)!= 0 and random.uniform(0, 1)>0.5:\n",
    "#             #topics\n",
    "#             for tid in lesson[\"topics\"]:\n",
    "#                 text = lesson[\"topics\"][tid]['content']['text']\n",
    "#                 relevent_vocabs = []\n",
    "\n",
    "#                 for vocab in vocabs:\n",
    "#                     start = text.find(vocab)\n",
    "#                     if start >=0: #vocab term found\n",
    "#                         relevent_vocabs.append(vocab)\n",
    "#                 if len(relevent_vocabs)>0 and random.uniform(0, 1)>0.5 and len(text_ans)<50: \n",
    "#                     text_ans.append((text, relevent_vocabs))\n",
    "\n",
    "# data=[('smith, bob',2),('carol',3),('ted',4),('alice',5)]\n",
    "\n",
    "# with open('textbook_context_vocab.csv','w') as f:\n",
    "#     writer=csv.writer(f)\n",
    "#     writer.writerow(['text','vocabs'])\n",
    "#     for row in text_ans:\n",
    "#         writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "text_vocabs = []\n",
    "\n",
    "with open('tqa_train_val_test/train/tqa_v1_train.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for lesson in data:\n",
    "        #one lesson\n",
    "        if len(text_vocabs)>=50:\n",
    "            break\n",
    "        #vocab\n",
    "        vocabs = list(lesson[\"adjunctTopics\"][\"Vocabulary\"].keys())\n",
    "        if len(vocabs)!= 0 and random.uniform(0, 1)>0.5:\n",
    "            #topics\n",
    "            for tid in lesson[\"topics\"]:\n",
    "                text = lesson[\"topics\"][tid]['content']['text']\n",
    "                ansIdx = []\n",
    "                for vocab in vocabs:\n",
    "                    start = text.find(vocab)\n",
    "                    if start >=0: #vocab term found\n",
    "                        end = start + len(vocab) # if the start_index is not -1\n",
    "                        word_start = len(sNLP.word_tokenize(text[:start]))\n",
    "                        word_end = word_start + len(sNLP.word_tokenize(text[start:end])) #inclusive\n",
    "                        ansIdx.append(list(range(word_start, word_end)))\n",
    "                if len(ansIdx)>0 and random.uniform(0, 1)>0.5 and len(text_vocabs)<50: \n",
    "                    text_vocabs.append((text, ansIdx))\n",
    "                    \n",
    "context_ans = [(sNLP.word_tokenize(x[0]), x[1][0])for x in text_vocabs]\n",
    "\n",
    "# print(json.dumps(person_dict, indent = 4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ans = []\n",
    "for x in text_vocabs:\n",
    "    for a in x[1]:\n",
    "        context_ans.append((sNLP.word_tokenize(x[0]), a))\n",
    "len(context_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_to_idx(context, answer): #retrun ans_idx\n",
    "# context = \"But when the Silk Road, the long overland trading route from China to the Mediterranean, became costlier and more dangerous to travel, Europeans searched for a more efficient and inexpensive trade route over water, initiating the development of what we now call the Atlantic World.\"\n",
    "# answer = \"Europeans searched for a more efficient and inexpensive trade route over water\"\n",
    "    start = context.find(answer)\n",
    "    if start >=0: #vocab term found\n",
    "        end = start + len(answer) # if the start_index is not -1\n",
    "        word_start = len(sNLP.word_tokenize(context[:start]))\n",
    "        word_end = word_start + len(sNLP.word_tokenize(context[start:end])) #inclusive\n",
    "        return(list(range(word_start, word_end)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
      "[27, 28]\n",
      "[27, 28]\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
      "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[5, 6]\n",
      "[28, 29]\n",
      "[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "#Tony and Manov model: given csv of context, modelanswer\n",
    "def preprocess_csv_context_answer(filename):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    text_ansidx = []\n",
    "    tokenized_text_ansidx = []\n",
    "    for i in range(len(df)):\n",
    "        df.columns = ['Context','modelanswer']\n",
    "        c = df['Context'][i]\n",
    "        a = df['modelanswer'][i]\n",
    "        ansIdx = answer_to_idx(c,a)\n",
    "        print(ansIdx)\n",
    "        if ansIdx != None: #if answer exists\n",
    "            text_ansidx.append((c, [ansIdx]))\n",
    "            tokenized_text_ansidx.append((sNLP.word_tokenize(c), ansIdx))\n",
    "    return text_ansidx, tokenized_text_ansidx\n",
    "\n",
    "text_ans, context_ans = preprocess_csv_context_answer('effect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'context_ans' (list)\n"
     ]
    }
   ],
   "source": [
    "%store context_ans\n",
    "del context_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n",
      "finding answers completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#given text_ans(text, ansIdx), create dataAns = list of dictionary(ex) that contains\n",
    "#document\n",
    "#pos\n",
    "#ner\n",
    "#doc_case\n",
    "#ansInd\n",
    "\n",
    "dataAns = []\n",
    "for text, ansIdxs in text_ans: #!!!!!!!!!CHANGE\n",
    "    ex = {}\n",
    "    document = sNLP.word_tokenize(text)\n",
    "    pos = [tup[1] for tup in sNLP.pos(text)]\n",
    "    ner = [tup[1] for tup in sNLP.ner(text)]\n",
    "    \n",
    "    ex['document'] = document\n",
    "    ex['pos'] = pos\n",
    "    ex['ner'] = ner\n",
    "    \n",
    "    \n",
    "    # get case info\n",
    "    doc_case = []\n",
    "    for w in ex['document']:\n",
    "        if w.isalpha():\n",
    "            if w.islower():\n",
    "                doc_case.append('L')\n",
    "            else:\n",
    "                doc_case.append('U')\n",
    "        else:\n",
    "            doc_case.append('L')\n",
    "    ex['doc_case'] = doc_case\n",
    "    \n",
    "    # find answers using special ner tags\n",
    "    ansInds = [] # list of lists of answer indicators\n",
    "#                 ansIdxs = [] #answer indexes\n",
    "\n",
    "#                 idx = 0\n",
    "#                 while idx < len(ner):\n",
    "#                     if ner[idx] != 'O':\n",
    "#                         k = idx+1\n",
    "#                         while k < len(ner):\n",
    "#                             if ner[k] == ner[idx]:\n",
    "#                                 k += 1\n",
    "#                             else:\n",
    "#                                 break\n",
    "#                         ansIdxs.append(list(range(idx,k)))\n",
    "#                         idx = k\n",
    "#                     else:\n",
    "#                         idx += 1    \n",
    "\n",
    "#                 new_ansIdxs = []# do not include duplicate answer indices\n",
    "#                 for ansIdx in ansIdxs:\n",
    "#                     if ansIdx not in new_ansIdxs:\n",
    "#                         new_ansIdxs.append(ansIdx)\n",
    "#                 ansIdxs = new_ansIdxs\n",
    "#                 print(\"ansindex\",ansIdxs)\n",
    "    \n",
    "    for idx in range(len(ansIdxs)): #for each answer, create answer indicaters\n",
    "        ansInd = ['-'] * len(ner)\n",
    "        for j in ansIdxs[idx]:\n",
    "            ansInd[j] = 'A'\n",
    "        ansInds.append(ansInd)\n",
    "        \n",
    "    for idx in range(len(ansInds)):#for each answer, creat ex and add\n",
    "        ansInd = ansInds[idx]\n",
    "        newData = deepcopy(ex)\n",
    "        newData['ansInd'] = ansInd\n",
    "#         print(\"ans\", idx, newData, '\\n')\n",
    "        dataAns.append(newData)\n",
    "    \n",
    "#     print(\"finding answers completed\")\n",
    "    \n",
    "    #end\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append case, pos, ner, ansInd as features to context\n",
    "with open(\"inputformat.txt\", 'wb') as f:\n",
    "    for ex in dataAns:\n",
    "        line = u' '.join([ex['document'][idx].replace(' ', '').lower() + '￨' + ex['doc_case'][idx] + '￨' +\n",
    "                        ex['pos'][idx] + '￨' + ex['ner'][idx] + '￨' + ex['ansInd'][idx]\n",
    "                        for idx in range(len(ex['document']))]).encode('utf-8').strip()\n",
    "        f.write(line + u'\\n'.encode('utf-8'))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer ||| context in readable format\n",
    "with open(\"readable.txt\", 'wb') as f:\n",
    "    for ex in dataAns:\n",
    "        ans = []\n",
    "        for idx in range(len(ex['ansInd'])):\n",
    "            if ex['ansInd'][idx] == 'A':\n",
    "                ans.append(ex['document'][idx])\n",
    "        line = u' '.join(ans + [' ||| '] + [ex['document'][idx].replace(' ', '').lower() \n",
    "                        for idx in range(len(ex['document']))]).encode('utf-8').strip()\n",
    "        f.write(line + u'\\n'.encode('utf-8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read output_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Europeans searched for a more efficient and inexpensive trade route over water ,  ', b'  globalization , the ever-increasing interconnectedness of the world , is not a new phenomenon , but it accelerated when western europeans discovered the riches of the east . during the crusades ( 1095 \\xe2\\x80\\x93 1291 ) , europeans developed an appetite for spices , silk , porcelain , sugar , and other luxury items from the east , for which they traded fur , timber , and slavic people they captured and sold ( hence the word slave ) . but when the silk road , the long overland trading route from china to the mediterranean , became costlier and more dangerous to travel , europeans searched for a more efficient and inexpensive trade route over water , initiating the development of what we now call the atlantic world .\\n']\n",
      "[b'Mistakenly believing they had reached the East Indies ,  ', b'  in pursuit of commerce in asia , fifteenth-century traders unexpectedly encountered a \\xe2\\x80\\x9c new world \\xe2\\x80\\x9d populated by millions and home to sophisticated and numerous peoples . mistakenly believing they had reached the east indies , these early explorers called its inhabitants indians . west africa , a diverse and culturally rich area , soon entered the stage as other nations exploited its slave trade and brought its peoples to the new world in chains . although europeans would come to dominate the new world , they could not have done so without africans and native peoples ( figure 1.1 ) .\\n']\n",
      "[b'Africans and native peoples  ', b'  in pursuit of commerce in asia , fifteenth-century traders unexpectedly encountered a \\xe2\\x80\\x9c new world \\xe2\\x80\\x9d populated by millions and home to sophisticated and numerous peoples . mistakenly believing they had reached the east indies , these early explorers called its inhabitants indians . west africa , a diverse and culturally rich area , soon entered the stage as other nations exploited its slave trade and brought its peoples to the new world in chains . although europeans would come to dominate the new world , they could not have done so without africans and native peoples ( figure 1.1 ) .\\n']\n"
     ]
    }
   ],
   "source": [
    "#read output\n",
    "import csv\n",
    "\n",
    "questions = open(\"output_questions_QG-Net.pt.txt.prob.txt\", 'rb')\n",
    "questions = questions.readlines() \n",
    "ans_context = open(\"readable.txt\", 'rb')\n",
    "ans_context = ans_context.readlines() \n",
    "scores = open(\"scores.csv\", 'wb')\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    row = []\n",
    "    row.append(questions[i])\n",
    "    \n",
    "    print(ans_context[i].split(('|||').encode('utf-8')))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
